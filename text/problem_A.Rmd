\section{Problem A: Comparing $AR(2)$ parameter estimators using resampling of residuals}

\subsection{1.}

First, we find the estimators $\hat{\bm{\beta}}_{LS}$ and $\hat{\bm{\beta}}_{LA}$ using the function `ARp.beta.est()`. Then, we calculate $\varepsilon_t$ for $t = 3, ..., T$ using the funciton `ARp.resid()`.

```{r}
# ####################################
# Find estimators for beta and epsilon
# ####################################
p <- 2
x <- data3A$x

betas <- ARp.beta.est(x, p)

beta_ls <- betas$LS
beta_la <- betas$LA

epsilon_ls <- ARp.resid(x, beta_ls)
epsilon_la <- ARp.resid(x, beta_la)


```


Now we use the residual resampling bootstrap method to evaluate $\hat{\bm{\beta}}_{LS}$ and $\hat{\bm{\beta}}_{LA}$. We use $B = 8000$ bootstrap samples of the residuals and simulate new time series based on these. $x_1$ and $x_2$ is chosen as two random consecutive variables from the observed time series for each iteration. New predictors $\hat{\bm{\beta}}_{LS}^{(b)}$ and $\hat{\bm{\beta}}_{LA}^{(b)}$ are calculated for each $b = 1, ..., B$. A histogram of the sampled estimators can be found in figure \@ref(fig:boot-hist).

```{r bootstrap}
```



(ref:fig-boot) Histograms displaying all samples of $\hat{\bm{\beta}}_{LS}^{(b)}$ and $\hat{\bm{\beta}}_{LA}^{(b)}$ from the residual resampling bootstrap method. $\hat{\beta}_1^{(b)}$ is displayed to the left for both loss functions, and $\hat{\beta}_2^{(b)}$ is displayed to the right. Estimators found using the LS loss function is displayed on the bottom and estimators found unsing the LA loss function is displayed on top.

```{r boot-hist, fig.cap = "(ref:fig-boot)"}
# ######################################################
# Use bootstrapping to resample the parameter estimators
# ######################################################

boot_iter <- 8000

boot_ls <- bootstrapA(x = x,
                       beta = beta_ls,
                       epsilon = epsilon_ls,
                       boot_iter = boot_iter,
                       type = "LS")



boot_la <- bootstrapA(x = x,
                       beta = beta_la,
                       epsilon = epsilon_la,
                       boot_iter = boot_iter,
                       type = "LA")

beta_mat_ls <- boot_ls$beta_mat
beta_mat_la <- boot_la$beta_mat



displayBeta(samples = beta_mat_la,
            samples_2 = beta_mat_ls,
            names = c("LA", "LS"),
            nbin = 70)

```

We can see from figure \@ref(fig:boot-hist) that the variance in $\hat{\bm{\beta}}_{LA}$ is much smaller than that of $\hat{\bm{\beta}}_{LS}$. The mean of the estimators seems to be approximately the same. We now try to estimate the bias and standard deviation of the estimators using the bootstrap samples. We estimate the standard deviation of $\hat{\bm{\beta}}$ by estimating the standard deviation of the samples $\hat{\bm{\beta}}^{(b)},\ b = 1, ..., B$. The bias of the estimators are found by subtracting $\hat{\bm{\beta}}$ from the mean of $\hat{\bm{\beta}}^{(b)}, \ b = 1, ..., B$. The numerical results can be seen in the print-out below. 

<!-- \begin{equation} -->
<!-- \widehat{SD(\hat{\bm{\beta}}_i) = \sqrt{\frac{1}{B - 1} \sum_{b = 1}^B }} -->
<!-- \end{equation} -->

```{r}
# #############################################################
# Estimate standard deviation and bias for parameter estimators
# #############################################################

sd_all <- data.frame(LA = c(sd(beta_mat_la[, 1]),
                            sd(beta_mat_la[, 2])),
                     LS = c(sd(beta_mat_ls[, 1]),
                            sd(beta_mat_ls[, 2])))

row.names(sd_all) <- c("sd_beta_1", "sd_beta_2")
format(sd_all, digits = 3)

bias_all <- data.frame(LA = c(mean(beta_mat_la[, 1]),
                            mean(beta_mat_la[, 2])) - beta_la,
                     LS = c(mean(beta_mat_ls[, 1]),
                            mean(beta_mat_ls[, 2])) - beta_ls)

row.names(bias_all) <- c("bias_beta_1", "bias_beta_2")
format(bias_all, digits = 3)

```

It seems that the LS estimator is not the optimal estimator for this non-Gaussian AR(2) process. This is due to the fact that the estimated variance is much smaller for the LA estimator. The absolute value of the bias is also much smaller for the LA estimator. We cannot rely very much on the bias, though. The standard deviation is much larger than the bias for both estimators, making the uncertainty of the bias estimates too big. \todo[inline]{This might need some more discussion?}


\subsection{2.}

We want to compute a $95 \%$ prediction interval for $x_{101}$. To do this we estimate $x_{101}^{(b)} = \hat{\beta}_1^{(b)} x_{100} + \hat{\beta}_2^{(b)} + \epsilon^{*(b)}$ for each bootstrap iteration. $\epsilon^{*(b)}$ is found by drawing uniformly from the original residual distribution used in problem $1.$. The prediction of $x_{101}$ is done in the function `bootstrapA()` shown earlier. Based on all samples of $x_{101}$ we make the prediction interval by removing the upper and lower quantiles of the $x_{101}$-values. The $2.5 \%$- and $97.5 \%$-quantiles from both estimators are shown below. 


```{r}
# ######################################
# Compute prediction intervals for x_101
# ######################################

x_pred_ls <- boot_ls$x_pred
x_pred_la <- boot_la$x_pred

alpha <- 0.05

q_ls <- quantile(x_pred_ls, probs = c(alpha / 2, 1 - alpha / 2))
q_la <- quantile(x_pred_la, probs = c(alpha / 2, 1 - alpha / 2))


cat("Lower and upper quantiles for the LS estimator:\n", q_ls)

cat("Lower and upper quantiles for the LA estimator:\n", q_la)





```

The size of the prediction intervals is approximately the same, and it is centered around the same values. This means that for prediction purposes, both estimators works equally well. One might have believed that the prediction interval based on the LS estimator should have been much smaller than that of the LS estimator. However, we can find that the standard deviations of the residual samples is equal to `r format(sd(epsilon_ls), digits = 3)` and `r format(sd(epsilon_la), digits = 3)` for the LS and LA estimator respectively. This is significantly larger than the standard deviation of the two estimators, and it will be the dominant term, giving approximately the same standard deviation to both predictors for $x_{101}$.


